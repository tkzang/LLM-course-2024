{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHG7XMqBgbT7rby6tEldfx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DHTCdGsFkmDj","executionInfo":{"status":"ok","timestamp":1731267305131,"user_tz":-120,"elapsed":7211,"user":{"displayName":"Zang","userId":"10239928199675251732"}},"outputId":"9bd895f1-7a45-4632-bf5d-de90f772d3bc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-5.1.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dT8X6mB7kfYt","executionInfo":{"status":"ok","timestamp":1731267307912,"user_tz":-120,"elapsed":550,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"outputs":[],"source":["import os\n","import requests\n","from bs4 import BeautifulSoup\n","import google.generativeai as genai\n","from urllib.request import urlopen, urlretrieve\n","from IPython.display import Markdown, display\n","from pypdf import PdfReader\n","from datetime import date\n","from tqdm import tqdm"]},{"cell_type":"code","source":["\n","genai.configure(api_key=\"AIzaSyC-n5pJ9rCi6_SC0weJiAnnFN8dU8HQhhA\")"],"metadata":{"id":"04mSuIBokivg","executionInfo":{"status":"ok","timestamp":1731267616735,"user_tz":-120,"elapsed":264,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["BASE_URL = \"https://huggingface.co/papers\"\n","page = requests.get(BASE_URL)\n","soup = BeautifulSoup(page.content, \"html.parser\")\n","h3s = soup.find_all(\"h3\")\n","\n","papers = []\n","\n","for h3 in h3s:\n","    a = h3.find(\"a\")\n","    title = a.text\n","    link = a[\"href\"].replace('/papers', '')\n","\n","    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"],"metadata":{"id":"z7bepLo_ksQv","executionInfo":{"status":"ok","timestamp":1731267621470,"user_tz":-120,"elapsed":507,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def extract_paper(url):\n","    html = urlopen(url).read()\n","    soup = BeautifulSoup(html, features=\"html.parser\")\n","\n","    # kill all script and style elements\n","    for script in soup([\"script\", \"style\"]):\n","        script.extract()    # rip it out\n","\n","    # get text\n","    text = soup.get_text()\n","\n","    # break into lines and remove leading and trailing space on each\n","    lines = (line.strip() for line in text.splitlines())\n","    # break multi-headlines into a line each\n","    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n","    # drop blank lines\n","    text = '\\n'.join(chunk for chunk in chunks if chunk)\n","\n","    return text\n","\n","\n","def extract_pdf(url):\n","    pdf = urlretrieve(url, \"pdf_file.pdf\")\n","    reader = PdfReader(\"pdf_file.pdf\")\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text() + \"\\n\"\n","    return text\n","\n","\n","def printmd(string):\n","    display(Markdown(string))"],"metadata":{"id":"7y13Q5eLkyaY","executionInfo":{"status":"ok","timestamp":1731267623060,"user_tz":-120,"elapsed":271,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["LLM = \"gemini-1.5-flash\"\n","model = genai.GenerativeModel(LLM)"],"metadata":{"id":"-1qYDlxUk0Um","executionInfo":{"status":"ok","timestamp":1731267626408,"user_tz":-120,"elapsed":297,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["for paper in tqdm(papers):\n","    try:\n","        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n","    except:\n","        print(\"Generation failed\")\n","        paper[\"summary\"] = \"Paper not available\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"353_58uRk2bR","executionInfo":{"status":"ok","timestamp":1731268074321,"user_tz":-120,"elapsed":447029,"user":{"displayName":"Zang","userId":"10239928199675251732"}},"outputId":"018cd685-dbd2-4544-87ba-6ad41d0d729b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [07:26<00:00, 24.82s/it]\n"]}]},{"cell_type":"code","source":["def extract_paper(url):\n","    html = urlopen(url).read()\n","    soup = BeautifulSoup(html, features=\"html.parser\")\n","\n","    # kill all script and style elements\n","    for script in soup([\"script\", \"style\"]):\n","        script.extract()    # rip it out\n","\n","    # get text\n","    text = soup.get_text()\n","\n","    # break into lines and remove leading and trailing space on each\n","    lines = (line.strip() for line in text.splitlines())\n","    # break multi-headlines into a line each\n","    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n","    # drop blank lines\n","    text = '\\n'.join(chunk for chunk in chunks if chunk)\n","\n","    return text\n","\n","\n","def extract_pdf(url):\n","    pdf = urlretrieve(url, \"pdf_file.pdf\")\n","    reader = PdfReader(\"pdf_file.pdf\")\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text() + \"\\n\"\n","    return text\n","\n","\n","def printmd(string):\n","    display(Markdown(string))"],"metadata":{"id":"yH84OZxBldLF","executionInfo":{"status":"ok","timestamp":1731268089326,"user_tz":-120,"elapsed":276,"user":{"displayName":"Zang","userId":"10239928199675251732"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["for paper in papers:\n","    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n","                                                paper[\"url\"],\n","                                                paper[\"summary\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ogTknTeoldvV","executionInfo":{"status":"ok","timestamp":1731268091357,"user_tz":-120,"elapsed":297,"user":{"displayName":"Zang","userId":"10239928199675251732"}},"outputId":"bcd4d422-f2c3-4d40-cd15-d2dea828baf6"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/pdf/2411.04905)**<br>OpenCoder is an open-source code large language model (LLM) that aims to bridge the performance gap between proprietary models and open-access models. Its strengths lie in its comprehensive release of training materials, including a reproducible pretraining dataset, a large-scale SFT corpus, and detailed training protocols. This transparency allows researchers to understand the model's development and build upon its foundation. OpenCoder achieves top-tier performance, surpassing most previous open-source models at both the 1B+ and 6B+ parameter scales. However, OpenCoder's development relies on extensive data filtering, including language-specific rules and deduplication, which can affect data diversity and potentially limit the model's generalizability. Additionally, while the model demonstrates strong performance on code-related tasks, its robustness across diverse languages and real-world scenarios needs further investigation. Nonetheless, OpenCoder offers a valuable resource for researchers to conduct reproducible experiments and accelerate advancements in code AI. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning](https://arxiv.org/pdf/2411.05003)**<br>This research paper presents ReCapture, a method for generating new videos with novel camera trajectories from existing user-provided videos. It leverages diffusion models to create a noisy \"anchor\" video with the desired camera movement and then employs a masked video fine-tuning technique to refine the anchor video, filling in missing regions and enhancing temporal consistency.  ReCapture achieves impressive results, surpassing existing approaches in both qualitative and quantitative evaluations, demonstrating its ability to generate high-quality videos with cinematic camera motion. However, the method relies on pre-trained video diffusion models and depth estimation techniques, which can limit its generalization capabilities to unseen data or scenes. Moreover, while the masked fine-tuning technique is effective, it can be computationally expensive and time-consuming.  Overall, ReCapture offers a promising approach for video re-angling, but further research is needed to address its limitations and enhance its efficiency and robustness. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/pdf/2411.04965)**<br>This research paper introduces BitNet a4.8, a novel hybrid quantization and sparsification strategy for enabling 4-bit activations in 1-bit Large Language Models (LLMs). It leverages the observation that activations in 1-bit LLMs exhibit a long-tailed distribution, with outliers causing significant quantization errors. BitNet a4.8 selectively applies 4-bit quantization to inputs to attention and feed-forward network (FFN) layers, where the distribution is Gaussian-like, while utilizing sparsification with 8 bits for intermediate states, where outliers are more prominent. This approach significantly reduces the computational cost of inference compared to previous 1-bit LLMs (e.g., BitNet b1.58) without compromising performance. BitNet a4.8 also supports 3-bit KV cache, further enhancing efficiency. While demonstrating promising results, the research acknowledges that further investigation into the impact of training tokens and activation distribution patterns on outlier prevalence is crucial for assessing scalability.  \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/pdf/2411.04996)**<br>This research paper introduces Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture designed to reduce computational costs in training large language models (LLMs) that process multiple modalities, like text, images, and speech. MoT achieves this by decoupling non-embedding parameters by modality, allowing modality-specific processing while maintaining global self-attention across the full input sequence. The study evaluates MoT across various multi-modal settings and model scales, demonstrating significant reductions in training FLOPs and wall-clock time while achieving comparable or better performance than dense baselines. MoT also shows improved performance compared to Mixture-of-Experts (MoE) models with the same FLOPs budget. However, MoT's performance gains are not consistent across all modalities, and combining modalities within a single tower can degrade performance. Furthermore, MoT's system performance depends heavily on the specific hardware and training environment. Overall, MoT demonstrates promise as a scalable and efficient architecture for multi-modal LLMs, but further research is needed to address its limitations and improve its performance in all modalities. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion](https://arxiv.org/pdf/2411.04928)**<br>DimensionX is a novel framework for generating 3D and 4D scenes from a single image using controllable video diffusion. It leverages the ST-Director, a module that decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This allows for precise manipulation of both spatial structure and temporal dynamics during video generation.  DimensionX also includes a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation, resulting in highly realistic and controllable outputs. While DimensionX surpasses previous methods in controllable video generation and 3D/4D scene creation, it still suffers from the limitations of the diffusion backbone, including difficulties with generating subtle details and the time-consuming inference process. Nonetheless, DimensionX demonstrates the potential of video diffusion models in creating dynamic and interactive environments. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation](https://arxiv.org/pdf/2411.04709)**<br>The research article introduces TIP-I2V, a large-scale dataset containing over 1.7 million user-provided text and image prompts specifically for image-to-video generation. This dataset provides a valuable resource for researchers studying user preferences and evaluating image-to-video models. TIP-I2V's strengths include its real-world nature, comprehensiveness, and inclusion of both text and image prompts. However, the dataset suffers from potential biases due to the uneven distribution of subjects and directions, and it does not include information about the users' identities or the specific image-to-video models they used. The authors demonstrate the dataset's potential by highlighting its application in user-oriented training, comprehensive evaluation, misinformation detection, and source image tracing. The study also highlights the challenges and opportunities for future research in developing more robust and ethical image-to-video models. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding](https://arxiv.org/pdf/2411.04952)**<br>This research paper presents M3D OCRAG, a multi-modal retrieval-augmented generation (RAG) framework for answering questions from multi-modal documents. M3D OCRAG addresses the limitations of existing methods by utilizing multi-modal retrieval to identify relevant document pages,  efficiently handling long documents and preserving visual information. The paper also introduces M3D OCVQA, a new open-domain benchmark dataset for evaluating DocVQA across 3,000+ PDF documents.  M3D OCRAG demonstrates superior performance compared to baselines in three benchmarks, including state-of-the-art results on MP-DocVQA. The paper's strengths include a novel framework, a new dataset, and comprehensive analysis. However, weaknesses lie in the reliance on English-heavy datasets and the potential for biased information. The framework's flexibility and potential for future improvement through component updates are acknowledged. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos](https://arxiv.org/pdf/2411.04923)**<br>This research paper introduces VideoGLaMM, a large multimodal model for pixel-level visual grounding in videos, which aims to address the shortcomings of existing video-based LMMs in precisely grounding textual responses to specific pixels in videos. The model utilizes a dual vision encoder to capture both spatial and temporal details from videos, integrating these features with a large language model through tunable adapters for close vision-language alignment. To facilitate training, the authors propose a new benchmark instruction-tuning dataset consisting of 38,000 video-QA triplets with detailed visual grounding annotations. While VideoGLaMM demonstrates superior performance compared to existing models across various tasks like grounded conversation generation, visual grounding, and referring video segmentation, it still has limitations such as potential noise in the dataset and struggles with objects of varying granularities. The paper showcases the promise of VideoGLaMM in enhancing video understanding through fine-grained visual grounding, but further development is required to address its limitations and improve its overall capabilities.\n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model](https://arxiv.org/pdf/2411.04496)**<br>This research article introduces THANOS, a family of large language models (LLMs) infused with \"skill-of-mind\" – the ability to understand conversational context and choose appropriate skills for responses. The authors introduce a new dataset, MULTIFACETED SKILL-OF-MIND, containing multi-turn conversations annotated with conversational skills and explanations.  THANOS, trained on this dataset, demonstrates strong generalizability in predicting conversational skills and generates high-quality explanations across various dialogue scenarios.  The authors show that integrating THANOS into LLM-based conversational agents significantly improves response quality and promotes prosocial behavior.  The strengths of this work lie in its introduction of a novel concept, skill-of-mind, and the construction of a large, diverse dataset with annotations for both skills and explanations. However, limitations include the potential for overfitting to the specific dataset and the need for further research to fully integrate skill-of-mind into dialogue agents. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/pdf/2411.05000)**<br>This research article investigates the ability of Large Language Models (LLMs) to handle long contexts by introducing challenging retrieval tasks. It evaluates 17 LLMs, including both open-source and closed-source models, on tasks involving single-needle retrieval, multiple-needle retrieval, conditional-needle retrieval, thread following, and multi-threading. The study finds that while many models are remarkably \"thread-safe,\" their accuracy decreases as the context length increases, revealing an \"effective\" context limit that is often significantly shorter than the supported context length. The authors also highlight the importance of comparing token counts from different tokenizers because they often correspond to substantially different numbers of written characters. They propose a task-specific effective context limit metric to assess the practical limits of each LLM. While this study offers valuable insights into long-context LLM capabilities, it is limited by its reliance on synthetic data and API restrictions that prevented extensive evaluation of some models. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[Analyzing The Language of Visual Tokens](https://arxiv.org/pdf/2411.05001)**<br>This research paper analyzes the statistical properties of \"visual languages\" created by tokenizing images into discrete patches, akin to words in natural language.  The authors find that while visual languages exhibit some similarities to natural languages, like following Zipfian distributions, they also exhibit key differences.  Visual languages have higher token innovation rates, leading to greater entropy and lower compression, suggesting more complex representations.  The authors further show that visual languages lack cohesive grammatical structures, resulting in higher perplexity and weaker hierarchical organization compared to natural languages.  Although vision models align more closely with natural languages than with other vision models, this alignment remains weaker than the cohesion within natural languages. The study highlights the need for modality-specific approaches in vision-language learning, emphasizing the unique statistical properties of visual languages. However, the research is limited by its reliance on VQ-VAE-based tokenizers and a specific scan order, suggesting further exploration of continuous tokenizers, different scan orders, and broader dataset coverage is needed to fully understand the complexities of visual language. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](https://arxiv.org/pdf/2411.05007)**<br>SVDQuant is a novel post-training quantization technique that enables 4-bit weights and activations for diffusion models, preserving visual fidelity and significantly accelerating inference. It addresses the limitations of traditional methods by introducing a low-rank branch to absorb outliers, effectively reducing quantization error. SVDQuant also includes an inference engine, Nunchaku, which fuses low-rank and low-bit computation kernels, minimizing memory overhead and maximizing speedup. The technique achieves a 3.5x memory reduction and 3.0x speedup on an NVIDIA RTX-4090 laptop compared to the 16-bit baseline, showcasing its potential for deploying large diffusion models on edge devices. However, SVDQuant currently lacks FP4 speedup results due to the unavailability of Blackwell GPUs.  Furthermore, the impact of increasing the rank of the low-rank branch needs further exploration.  Overall, SVDQuant presents a promising approach for accelerating diffusion models, offering a significant step towards enabling interactive AI applications on resource-constrained devices. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation](https://arxiv.org/pdf/2411.04999)**<br>This research paper introduces DynaMem, a novel dynamic spatio-semantic memory for open-world mobile manipulation. DynaMem addresses the limitations of existing systems that assume static environments by constructing a 3D data structure that maintains a dynamic memory of point clouds, enabling robots to explore and adapt to changing environments. The system can localize objects and navigate using either embedded vision-language features or multimodal LLMs, offering a hybrid approach that combines the strengths of both. Experimental results demonstrate that DynaMem achieves a significant improvement in pick-and-drop success rates compared to static systems, particularly in dynamic scenarios. However, the paper acknowledges the need for further development, highlighting limitations such as reliance on pre-trained models and a high failure rate in manipulation tasks. Despite these limitations, DynaMem represents a significant advancement in open-world mobile manipulation, paving the way for more robust and adaptable robotic systems. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[GazeGen: Gaze-Driven User Interaction for Visual Content Generation](https://arxiv.org/pdf/2411.04335)**<br>GazeGen is a novel user interaction system that generates visual content, like images and videos, based on the user's eye gaze. The system's strength lies in its ability to manipulate visual content intuitively by targeting regions of interest with gaze, making it particularly beneficial for people with physical disabilities. At its core is the DFT Gaze agent, a lightweight model with only 281K parameters that performs real-time gaze predictions tailored to individual users.  DFT Gaze leverages knowledge distillation and personalization techniques to achieve high accuracy with minimal computational resources. However, GazeGen suffers from limitations in its ability to handle glare and closed eyes, and its visual content generation struggles to accurately reflect 3D angles and orientations. Despite these weaknesses, GazeGen showcases the potential of gaze-driven interfaces for visual content creation, paving the way for more intuitive and accessible user experiences. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models](https://arxiv.org/pdf/2411.05005)**<br>This research article proposes a novel framework called Diff-2-in-1, which utilizes diffusion models for both multi-modal data generation and dense visual perception tasks.  Diff-2-in-1 bridges the gap between generative and discriminative learning by employing a self-improving mechanism with two sets of parameters.  One set generates synthetic data, while the other utilizes both real and synthetic data to learn discriminative tasks, continuously improving the quality of generated data.  The framework shows promising results across various tasks, demonstrating consistent performance improvements and the ability to generate realistic and useful multi-modal data. However, a major limitation is the time-consuming data generation process due to the multi-step denoising required by diffusion models.  Future research directions include extending this framework to other tasks, optimizing data generation schemes, and enabling knowledge transfer to new domains. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval](https://arxiv.org/pdf/2411.04752)**<br>This research paper investigates the retrieval of relevant information from code-mixed conversations, particularly focusing on Roman transliterated Bengali mixed with English. The authors address the challenges posed by the informal and non-standardized nature of this type of language. Their approach leverages the GPT-3.5 Turbo model through prompting, utilizing the sequential nature of documents to frame a mathematical model that enhances relevance detection. While the paper highlights the strengths of this novel approach, it also acknowledges limitations, such as the need for further improvement in the GPT-3.5 model's performance. The research contributes to the broader field of natural language processing, especially for multilingual and informal text environments, by providing a valuable solution for extracting relevant information from code-mixed conversations. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](https://arxiv.org/pdf/2411.04989)**<br>This research paper introduces SG-I2V, a self-guided framework for controllable image-to-video generation.  It uses a pre-trained image-to-video diffusion model and doesn't require fine-tuning or external knowledge.  The model achieves control by analyzing and aligning feature maps from the self-attention layers of the model, then optimizing the latent code to encourage feature similarity within bounding boxes.  However, this optimization can degrade video quality, so the model incorporates frequency-based post-processing to preserve high-frequency noise and improve video quality.  SG-I2V outperforms zero-shot baselines and is competitive with supervised models in terms of visual quality and motion fidelity, but it is limited by the quality of the pre-trained model and the potential for artifacts during optimization.  The authors advocate for safe and responsible use of generative models like SG-I2V, as they have the potential to create photorealistic videos that could be misused. \n<br><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**[M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models](https://arxiv.org/pdf/2411.04075)**<br>This research article introduces M3S CIQA, a multi-modal, multi-document scientific question answering benchmark designed to evaluate the capabilities of foundation models in handling the complex information retrieval and reasoning tasks encountered in real-world scientific research. The benchmark consists of 1,452 expert-annotated questions spanning 70 NLP paper clusters, requiring models to interpret figures and tables, gather information across multiple documents, and perform diverse reasoning tasks. Evaluation of various foundation models on M3S CIQA reveals significant performance gaps between these models and human experts, especially in areas like scientific image understanding, long-range retrieval, and multi-document analysis. While the benchmark is a valuable resource for evaluating foundation models, it has limitations such as inconsistencies in context window sizes across models and the potential for prompt ambiguity to influence ranking results. Future work could focus on standardizing context window sizes, exploring alternative approaches to ranking papers, and developing specialized foundation models trained on scientific data for improved performance in scientific applications. \n<br><br>"},"metadata":{}}]}]}